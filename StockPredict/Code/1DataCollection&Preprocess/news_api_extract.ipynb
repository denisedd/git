{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction on Guardian news\n",
    "1. From the returned NewsAPI dataset, we only get the dataset with published date, headline, and url of news.\n",
    "2. With beautiful soup, scrape the informative content of the whole news article\n",
    "3. The news source include CNN, Reuters, Washington news, BBC, Bloomberg, The Wall Street Journal\n",
    "3. all extracted news are stored in respective dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start crawling The Wall Street Journal news\n",
      "finish crawling The Wall Street Journal news\n",
      "start crawling bloomberg news\n",
      "finish crawling bloomberg news\n",
      "start crawling BBC news\n",
      "finish crawling BBC news\n",
      "start crawling washington news\n",
      "finish crawling washington news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/denise/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:125: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start crawling cnn news\n",
      "finish crawling cnn news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/denise/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def process_date(date):\n",
    "    \n",
    "    if len(date.split(' ')) < 3:\n",
    "        return date+' 2019'\n",
    "    elif len(date.split(' ')) > 3:\n",
    "        return ' '.join(date.split(' ')[-3:])\n",
    "    else:\n",
    "        return date\n",
    "    \n",
    "def process_time(time):\n",
    "    if 'Updated' in time:\n",
    "        return ' '.join(time.strip().split(' ')[1:4])\n",
    "    else:\n",
    "        return ' '.join(time.strip().split(' ')[:3])\n",
    "\n",
    "def date_format(date):\n",
    "    DATE_FORMATS = ['%B %d, %Y', '%b. %d, %Y', '%b %d, %Y', '%B %d %Y']\n",
    "    for a in DATE_FORMATS:\n",
    "        try:\n",
    "            date_time = pd.to_datetime(date, format = a)\n",
    "        except:\n",
    "            continue\n",
    "    return date_time\n",
    "\n",
    "def several_news(soup):\n",
    "    regex = re.compile('.*timeDelta.*')\n",
    "    lst1, lst2, lst3 = ([] for i in range(3))\n",
    "    headline = soup.find_all(\"h2\")[2:]\n",
    "    time = soup.find_all(\"span\", {'class': regex})\n",
    "    for aa in headline:\n",
    "        content = aa.findNext('div')\n",
    "        content_1 = content.find_all(\"p\")\n",
    "        content_2 = ' '.join([a.text for a in content_1])\n",
    "        lst3.append(content_2)\n",
    "    for a, b in zip(headline, time):\n",
    "        lst1.append(a.text)\n",
    "        lst2.append(''.join(b.text.split('ET, ')[1:]))\n",
    "    return lst1, lst2, lst3\n",
    "\n",
    "def cnn_crawl(cnn_lst, path):\n",
    "    print('start crawling cnn news')\n",
    "    headline_lst, date_lst, all_content_lst = ([] for i in range(3))\n",
    "    for file in os.listdir(path):\n",
    "        if file in cnn_lst:\n",
    "            with open(path+file, 'r') as news:\n",
    "                data = news.read()\n",
    "            soup = BeautifulSoup(data, features=\"html.parser\")\n",
    "            try:\n",
    "                headline = soup.find(\"h1\", {\"class\": \"pg-headline\"}).get_text()\n",
    "            except:\n",
    "                lst1, lst2, lst3 = several_news(soup)\n",
    "                headline_lst = lst1+headline_lst\n",
    "                date_lst = lst2+date_lst\n",
    "                all_content_lst = lst3+all_content_lst\n",
    "                continue\n",
    "            time = soup.find(\"p\", {\"class\": \"update-time\"}).get_text()\n",
    "            time1 = ''.join(time.split(',')[-2:])\n",
    "            time_date = ' '.join(time1.split(' ')[2:-1])\n",
    "            time_date = process_date(time_date)\n",
    "            regex = re.compile('^zn-body__paragraph')\n",
    "            try:\n",
    "                content = soup.find(\"div\", {\"class\": \"pg-rail-tall__body\"})\n",
    "                content_head = content.find(\"p\").get_text()\n",
    "                content_rr = content.find_all(\"div\", {\"class\": regex})\n",
    "            except:\n",
    "                content = soup.find(\"div\", {\"class\": \"pg-special-article__body\"})\n",
    "                try:\n",
    "                    content_head = content.find(\"p\", {\"class\": regex}).get_text()\n",
    "                    content_rr = content.find_all(\"div\", {\"class\": regex})\n",
    "                except:\n",
    "                    content = soup.find(\"div\", {\"class\": \"pg-special-article__wrapper\"})\n",
    "                    content_head = content.find(\"div\", {\"class\": 'pg-special-article__body'}).get_text()\n",
    "                    content_rr = content.find_all(\"p\", {\"class\": regex})\n",
    "\n",
    "            content_lst = [a.text for a in content_rr]\n",
    "            all_content = ' '.join(content_lst)\n",
    "            cnn_content = content_head+' '+all_content\n",
    "            headline_lst.append(headline)\n",
    "            date_lst.append(time_date)\n",
    "            all_content_lst.append(cnn_content)\n",
    "            \n",
    "    print('finish crawling cnn news')        \n",
    "    CNN_news = pd.DataFrame({\n",
    "            'headline': headline_lst,\n",
    "            'date': date_lst,\n",
    "            'content': all_content_lst})\n",
    "    CNN = CNN_news.loc[(CNN_news['date'].str.len() > 3) & (CNN_news['headline'].str.len() > 3)]\n",
    "    CNN['date'] = CNN['date'].apply(date_format)\n",
    "    return CNN\n",
    "        \n",
    "def reuters_crawl(reu_news, path):\n",
    "    print('start crawling reuters news')\n",
    "    headline_lst, date_lst, all_content_lst = ([] for i in range(3))\n",
    "    for file in os.listdir(path):\n",
    "        if file in reu_news:\n",
    "\n",
    "            with open(path+file, 'r') as news:\n",
    "                data = news.read()\n",
    "            soup = BeautifulSoup(data, features=\"html.parser\")\n",
    "            try:\n",
    "                headline = soup.find(\"h1\", {\"class\": \"ArticleHeader_headline\"}).get_text()\n",
    "            except:\n",
    "                continue\n",
    "            time = soup.find(\"div\", {\"class\": \"ArticleHeader_date\"}).get_text()\n",
    "            time_date = time.split('/')[0]\n",
    "            content_div = soup.find(\"div\", {\"class\": \"StandardArticleBody_body\"})\n",
    "            content = content_div.find_all(\"p\")\n",
    "            content_lst = [tag.text for tag in content if len(tag.attrs) < 1]\n",
    "            all_content = ' '.join(content_lst)\n",
    "            headline_lst.append(headline)\n",
    "            date_lst.append(time_date[:-1])\n",
    "            all_content_lst.append(all_content)\n",
    "    Reuters_news1 = pd.DataFrame({\n",
    "            'headline': headline_lst,\n",
    "            'date': date_lst,\n",
    "            'content': all_content_lst})\n",
    "    Reuters_news = Reuters_news1.loc[Reuters_news1['content'].str.len() > 3]\n",
    "    Reuters_news['date'] = pd.to_datetime(Reuters_news['date'], format='%B %d, %Y')\n",
    "    print('finish crawling reuters news')\n",
    "    return Reuters_news\n",
    "            \n",
    "def washington_crawl(washin_news, path):\n",
    "    print('start crawling washington news')\n",
    "    headline_lst, date_lst, all_content_lst = ([] for i in range(3))\n",
    "    filter_string=['RELATED LINKS', 'And here are a few more good reads', 'Coming Up', 'Email', 'https://', 'http://', 'AP Photo', 'Coming soon']\n",
    "    for file in os.listdir(path):\n",
    "        if file in washin_news:\n",
    "\n",
    "            with open(path+file, 'r') as news:\n",
    "                data = news.read()\n",
    "            soup = BeautifulSoup(data, features=\"html.parser\")\n",
    "            try:\n",
    "                headline_div = soup.find(\"div\", {\"class\": \"topper-headline\"})\n",
    "                headline = headline_div.find('h1').get_text()\n",
    "            except:\n",
    "                continue\n",
    "            time_div = soup.find(\"div\", {\"id\": \"article-body\"})\n",
    "            time = time_div.find('span', {\"class\": \"author-timestamp\"})['content']\n",
    "            time_date = time.split('T')[0]\n",
    "            content_div = soup.find(\"div\", {\"id\": \"article-body\"})\n",
    "            content = content_div.find_all(\"p\")\n",
    "            content_lst = [tag.text for tag in content if (len(tag.attrs) <= 1) and (not tag.text.startswith(\"Copyright\")) and not any (e in tag.text for e in filter_string)]\n",
    "            all_content = ' '.join(content_lst)\n",
    "\n",
    "            headline_lst.append(headline)\n",
    "            date_lst.append(time_date)\n",
    "            all_content_lst.append(all_content)\n",
    "    Washington_news = pd.DataFrame({\n",
    "            'headline': headline_lst,\n",
    "            'date': date_lst,\n",
    "            'content': all_content_lst})\n",
    "    print('finish crawling washington news')\n",
    "    Washington_news['date'] = pd.to_datetime(Washington_news['date'])\n",
    "    return Washington_news        \n",
    "            \n",
    "def BBC_crawl(bbc_news, path):\n",
    "    print('start crawling BBC news')\n",
    "    headline_lst, date_lst, all_content_lst = ([] for i in range(3))\n",
    "    filter_string = 'Text by'\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if file in bbc_news:\n",
    "\n",
    "            with open(path+file, 'r') as news:\n",
    "                data = news.read()\n",
    "            soup = BeautifulSoup(data, features=\"html.parser\")\n",
    "            try:\n",
    "                headline = soup.find(\"h1\", {\"class\": \"story-body__h1\"}).get_text()\n",
    "            except:\n",
    "                continue\n",
    "            time = soup.find(\"div\", {\"class\": \"date date--v2\"}).get_text()\n",
    "            content_div = soup.find(\"div\", {\"class\": \"story-body__inner\"})\n",
    "            content_p = content_div.find_all(\"p\")\n",
    "            content= ''.join([tag.text for tag in content_p if not filter_string in tag.text])\n",
    "            headline_lst.append(headline)\n",
    "            date_lst.append(time)\n",
    "            all_content_lst.append(content)\n",
    "    BBC_news = pd.DataFrame({\n",
    "            'headline': headline_lst,\n",
    "            'date': date_lst,\n",
    "            'content': all_content_lst})\n",
    "    print('finish crawling BBC news')\n",
    "    BBC_news['date'] = pd.to_datetime(BBC_news['date'], format='%d %B %Y')\n",
    "    return BBC_news\n",
    "\n",
    "def bloomberg_crawl(bloom_news, path):\n",
    "    print('start crawling bloomberg news')\n",
    "    headline_lst, date_lst, all_content_lst = ([] for i in range(3))\n",
    "    filter_string = ['Illustration', 'Photographer', \"@bloomberg.net\"]\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if file in bloom_news:\n",
    "\n",
    "            with open(path+file, 'r') as news:\n",
    "                data = news.read()\n",
    "            soup = BeautifulSoup(data, features=\"html.parser\")\n",
    "            try:\n",
    "                time = soup.find(\"time\", {\"class\": \"article-timestamp\"})['datetime']\n",
    "            except:\n",
    "                continue\n",
    "            time_date = time.split('T')[0]\n",
    "            try:\n",
    "                headline = soup.find(\"h1\", {\"class\": \"lede-text-v2__hed\"}).get_text()\n",
    "                content_div = soup.find(\"div\", {\"class\": \"body-copy-v2 fence-body\"})\n",
    "                content = content_div.find_all(\"p\")\n",
    "                content_head = ''.join([tag.text for tag in content if not any (e in tag.text for e in filter_string)])\n",
    "                try:\n",
    "                    header_div = soup.find_all(\"div\", {\"class\": \"abstract-v2__item-text\"})\n",
    "                    sub_header = '. '.join([tag.text.strip() for tag in header_div])\n",
    "                except:\n",
    "                    sub_header=''\n",
    "                try:\n",
    "                    sub_content_li = content_div.find_all(\"li\")\n",
    "                    sub_content = ''.join([tag.text for tag in sub_content_li])\n",
    "                except:\n",
    "                    sub_content=''\n",
    "                cont = sub_header+' '+content_head+' '+sub_content\n",
    "\n",
    "            except:\n",
    "                try:\n",
    "                    headline = soup.find(\"h1\", {\"class\": \"lede-text-only__hed\"}).get_text()\n",
    "                    content_div = soup.find(\"div\", {\"class\": \"body-columns\"})\n",
    "                    content = content_div.find_all(\"p\")\n",
    "                    content_head = ''.join([tag.text for tag in content if not any (e in tag.text for e in filter_string)])\n",
    "                    try:\n",
    "                        bottom_content = content_div.find(\"span\", {\"class\": \"bottom-line__text\"}).get_text()\n",
    "                    except:\n",
    "                        bottom_content=''\n",
    "                    cont = content_head+' '+bottom_content\n",
    "                except:\n",
    "                    continue\n",
    "            headline_lst.append(headline)\n",
    "            date_lst.append(time_date)\n",
    "            all_content_lst.append(cont)\n",
    "    print('finish crawling bloomberg news')       \n",
    "    Bloomberg_news = pd.DataFrame({\n",
    "            'headline': headline_lst,\n",
    "            'date': date_lst,\n",
    "            'content': all_content_lst})\n",
    "    Bloomberg_news['date'] = pd.to_datetime(Bloomberg_news['date'])\n",
    "    return Bloomberg_news\n",
    "\n",
    "def WSJ_crawl(wsj_news, path):\n",
    "    print('start crawling The Wall Street Journal news')\n",
    "    regex = re.compile('timestamp.*')\n",
    "    filter_string = ['@wsj.com', 'contributed to this article', 'Please email']\n",
    "    headline_lst, date_lst, all_content_lst = ([] for i in range(3))\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if file in wsj_news:\n",
    "\n",
    "            with open(path+file, 'r') as news:\n",
    "                data = news.read()\n",
    "            soup = BeautifulSoup(data, features=\"lxml\")\n",
    "            try:\n",
    "                headline = soup.find(\"h1\", {\"itemprop\": \"headline\"}).get_text().strip()\n",
    "            except:\n",
    "                continue\n",
    "            time = soup.find(\"time\", {\"class\": regex}).get_text()\n",
    "            time_date = process_time(time)\n",
    "            try:\n",
    "                header = soup.find('h2', {'class': 'sub-head'}).get_text().strip()\n",
    "            except:\n",
    "                try:\n",
    "                    header = soup.find('h2', {'itemprop': 'description'}).get_text().strip()\n",
    "                except:\n",
    "                    pass\n",
    "            try:\n",
    "                content_div = soup.find(\"div\", {\"class\": \"article-content\"})\n",
    "                content = content_div.find_all(\"p\")\n",
    "                head = ''\n",
    "            except:\n",
    "                try:\n",
    "                    content_div = soup.find(\"div\", {\"class\": 'paywall'})\n",
    "                    head = content_div.find_previous_sibling('p').get_text()\n",
    "                    content = content_div.find_all(\"p\")\n",
    "                except:\n",
    "                    continue\n",
    "            content_lst = [tag.text for tag in content if not any (a in tag.text for a in filter_string)]\n",
    "            all_content = ' '.join(content_lst)\n",
    "            combined_content = header+' '+head+' '+all_content\n",
    "\n",
    "            headline_lst.append(headline)\n",
    "            date_lst.append(time_date)\n",
    "            all_content_lst.append(combined_content)\n",
    "        \n",
    "    WSJ_news = pd.DataFrame({\n",
    "            'headline': headline_lst,\n",
    "            'date': date_lst,\n",
    "            'content': all_content_lst\n",
    "     })\n",
    "\n",
    "    WSJ_news['date'] = WSJ_news['date'].apply(date_format)\n",
    "    print('finish crawling The Wall Street Journal news')\n",
    "    return WSJ_news\n",
    "\n",
    "def get_news_lst(path):\n",
    "#     with open('Data/NewsAPI/finance/index.json', 'r') as f:\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    aurl, burl, curl, durl, eurl, furl, gurl = ([] for i in range(7))\n",
    "    news_dict = dict.fromkeys(['source_news','url'])\n",
    "    for news in data['articles']:\n",
    "        if news['source']['name'] == 'CNN':\n",
    "            html = news['url'].split('https://')[1]\n",
    "            html1 = html.replace('/',':')\n",
    "            aurl.append(html1)\n",
    "            \n",
    "        elif news['source']['name'] == 'Reuters':\n",
    "            html = news['url'].split('https://')[1]\n",
    "            html1 = html.replace('/',':')\n",
    "            burl.append(html1)\n",
    "        \n",
    "        elif news['source']['name'] == 'The Washington Post':\n",
    "            try:\n",
    "                html = news['url'].split('https://')[1]\n",
    "            except:\n",
    "                html = news['url'].split('http://')[1]\n",
    "            html1 = html.replace('/',':')\n",
    "            curl.append(html1)\n",
    "        \n",
    "        elif news['source']['name'] == 'BBC News':\n",
    "            html = news['url'].split('https://')[1]\n",
    "            html1 = html.replace('/',':')\n",
    "            durl.append(html1)\n",
    "        \n",
    "        elif news['source']['name'] == 'Bloomberg':\n",
    "            html = news['url'].split('https://')[1]\n",
    "            html1 = html.replace('/',':')\n",
    "            eurl.append(html1)\n",
    "        \n",
    "        elif news['source']['name'] == 'The Wall Street Journal':\n",
    "            html = news['url'].split('https://')[1]\n",
    "            html1 = html.replace('/',':')\n",
    "            furl.append(html1)\n",
    "        \n",
    "        else:\n",
    "            try:\n",
    "                html = news['url'].split('http://')[1]\n",
    "            except:\n",
    "                html = news['url'].split('https://')[1]\n",
    "            html1 = html.replace('/',':')\n",
    "            gurl.append(html1) # Google News\n",
    "    return aurl, burl, curl, durl, eurl, furl\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path = os.path.join('../../Data', 'NewsAPI/')\n",
    "    cnn_lst, reu_news, washin_news, bbc_news, bloom_news, wsj_news = get_news_lst(path+'stock/index.json')\n",
    "    path = os.path.join('../../Data', 'NewsAPI', 'stock', 'WebPage/')\n",
    "    WSJ_news_df = WSJ_crawl(wsj_news, path)\n",
    "    bloomberg_news_df = bloomberg_crawl(bloom_news, path)\n",
    "    bbc_news_df = BBC_crawl(bbc_news, path)\n",
    "    washin_news_df = washington_crawl(washin_news, path)\n",
    "    reuters_news_df = reuters_crawl(reu_news, path)\n",
    "    cnn_news_df = cnn_crawl(cnn_lst, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
