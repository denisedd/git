{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Regression model\n",
    "1. read news dataset and caluculate sentiment of headline\n",
    "2. compue the mean of sentiment score on news headline each day\n",
    "3. concatenate the data set with stock dataset to get stcok value\n",
    "4. normalize stocks value with MinMAxscaler\n",
    "5. Fit the sentiment score and normalized stock price into the model\n",
    "6. compare the model performance with different windows size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quandl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'mean_squared_error', 'mean_absolute_error', 'mean_absolute_percentage_error', 'cosine_proximity']\n",
      "262/262 [==============================] - 0s 201us/step\n",
      "[0.0005391922518113301, 0.0005391922518113301, 0.01776900385570435, 2.210916217956834, -1.0]\n"
     ]
    }
   ],
   "source": [
    "import quandl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "#\n",
    "\n",
    "def stocks(start_date, end_date):\n",
    "# use quandl to acquire nasdaq composite\n",
    "    ndq = quandl.get(\"NASDAQOMX/COMP-NASDAQ\",\n",
    "                        start_date = start_date, \n",
    "                        end_date = end_date)\n",
    "    ndq_df = ndq.reset_index()\n",
    "    val = ndq_df['Index Value'].tolist()\n",
    "    return ndq_df, val\n",
    "\n",
    "def concat_headline(df):\n",
    "    result_1 = df[['date', 'sentiment']]\n",
    "    result_2 = result_1.groupby('date').cumcount() + 1\n",
    "    result_3 = result_1.set_index(['date', result_2]).unstack().sort_index(1, level=1)\n",
    "    result_3.columns = ['_'.join(map(str,i)) for i in result_3.columns]\n",
    "    result_3 = result_3.reset_index()\n",
    "    result_3['date'] = pd.to_datetime(result_3['date'])\n",
    "    return result_3\n",
    "\n",
    "def process(data):\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    df = data.sort_values(by='date')\n",
    "    df_1 = df.loc[(df['date'] >= '20140101') & (df['date'] <= '2019-04-04')]\n",
    "    df_1['headline'] = df_1['headline'].fillna(\" \")\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    df_1['sentiment'] = df_1['headline'].apply(lambda x:sid.polarity_scores(x)['compound'] if (len(x)>3) else float(0))\n",
    "    df_2 = concat_headline(df_1)\n",
    "    return df_2\n",
    "\n",
    "def senti_mean(df_2):\n",
    "    # calculate sentiment mean of headline from news every day\n",
    "    ttl=[]\n",
    "    for row in range(df_2.shape[0]):\n",
    "        lst=[]\n",
    "        for col in range(1, df_2.shape[1]):\n",
    "            if (df_2.iloc[row, col] is not None) or (pd.notnull(df_2.iloc[row, col])):\n",
    "                    lst.append(df_2.iloc[row, col].astype(float))\n",
    "        cleanedList = [x for x in lst if str(x) != 'nan']\n",
    "        arr_mean = np.mean(cleanedList, axis=0)\n",
    "        ttl.append(arr_mean)\n",
    "    return ttl\n",
    "\n",
    "def min_max(df_2):\n",
    "    stocks_df1, val = stocks('2014-01-01', '2019-04-04')\n",
    "    stocks_df2 = stocks_df1[['Trade Date', 'Index Value']]\n",
    "    aa = stocks_df2.merge(df_2, how='left', left_on='Trade Date', right_on='date')\n",
    "    \n",
    "    # MinaxScaler\n",
    "    stock_value = aa['Index Value'].values.astype('float32')\n",
    "    Nas_stock_prices = stock_value.reshape(aa.shape[0], 1)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    Nas_stock_prices_1 = scaler.fit_transform(Nas_stock_prices)\n",
    "    aa['min_max_stock'] = Nas_stock_prices_1\n",
    "    aa['Label'] = aa['min_max_stock'].shift(-1)\n",
    "    aa = aa.iloc[:-1, :]\n",
    "    return aa, scaler\n",
    "\n",
    "def processData(data,lb):\n",
    "    # chunk data\n",
    "    X,Y = [],[]\n",
    "    for i in range(data.shape[0]-lb-1):\n",
    "        X.append(data[i:(i+lb),:])\n",
    "        Y.append(data[(i+lb),:])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def train_data(aa):\n",
    "    ttl = []\n",
    "    for row in range(aa.shape[0]):\n",
    "        lst = []\n",
    "        lst.append(float(aa.iloc[row,-3]))\n",
    "        lst.append(float(aa.iloc[row,-2]))\n",
    "        lst.append(float(aa.iloc[row,-1]))\n",
    "        ttl.append(lst)\n",
    "    return ttl\n",
    "\n",
    "def lstm_model(x_train, y_train, window_size):\n",
    "    \n",
    "    filepath=\"model/model_15.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=0, \n",
    "                             save_best_only=True)\n",
    "    \n",
    "    #Build the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(window_size, x_train.shape[-1])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam',loss='mse', metrics=['mse', 'mae', 'mape', 'cosine'])\n",
    "\n",
    "    #Fit model with history to check for overfitting\n",
    "    history = model.fit(x_train, y_train, epochs=500, batch_size=50\n",
    "                        ,validation_split=0.1,verbose=0, shuffle=False, callbacks=[checkpoint])\n",
    "    return model\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_pickle('../..Data/ProcessedNews.pkl')\n",
    "    df_2 = process(data)\n",
    "    ttl = senti_mean(df_2)\n",
    "    df_2['average_senti'] = ttl\n",
    "    aa, scaler = min_max(df_2)\n",
    "    ttl = train_data(aa)\n",
    "    arr_data = np.asarray(ttl)\n",
    "    window_size = 15\n",
    "    X,y = processData(arr_data, window_size)\n",
    "    x_train,x_test = X[:int(X.shape[0]*0.80), :, :-1],X[int(X.shape[0]*0.80):, :, :-1]\n",
    "    y_trai n,y_test = y[:int(y.shape[0]*0.80), -2],y[int(y.shape[0]*0.80):, -2]\n",
    "    model = lstm_model(x_train, y_train, window_size)\n",
    "    \n",
    "#     Xt = model.predict(x_test)\n",
    "#     print(scaler.inverse_transform(Xt))\n",
    "    print(model.metrics_names)\n",
    "    print(model.evaluate(x_test,y_test))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['loss', 'mean_squared_error', 'mean_absolute_error']\n",
    "#### window size 15\n",
    "# \n",
    "#### window size 20\n",
    "# [0.0013658519971659015, 0.0013658519971659015, 0.0329189677633545]\n",
    "#### window size 30\n",
    "# [0.0005190663024401976, 0.0005190663024401976, 0.016322801936000702]\n",
    "#### window size 45\n",
    "# [0.0013566281922976486, 0.0013566281922976486, 0.03261727234348655]\n",
    "# window size 50\n",
    "# [0.0007157920065390713, 0.0007157920065390713, 0.021968317645437576, 2.665913009176067, -1.0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
