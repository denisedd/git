{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "1. Firstly, we transform the headline through CountVectorizer with 1-ngram, 2-ngram, and 3-ngram model\n",
    "2. Fit the n-gram embedding into Linear Support Vector Machine to predict the stock trend whether it would rise or drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/denise/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "import quandl\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_wrd = set(stopwords.words('english'))\n",
    "\n",
    "def filter_sec(df):\n",
    "    filter_cate = ['N.Y. / Region', 'Todayâ€™s Paper', 'Multimedia/Photos', 'U.S.', 'Reader Center', 'Magazine', 'Technology',\n",
    "    'Opinion', 'International Home', 'Job Market', 'Briefing', 'Education', 'World', 'Business', 'Your Money',\n",
    "    'NYT Now', 'Podcasts', 'New York', 'Real Estate', 'Sunday Review', 'Business Day', 'Neediest Cases''T Magazine', 'Times Insider',\n",
    "     'Public Editor', 'Universal'] \n",
    "    df_1 = df[df.section.isin(filter_cate)]\n",
    "    return df_1\n",
    "\n",
    "def index_fluc(value):\n",
    "    if value >0:\n",
    "        return 1\n",
    "    elif value <0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def read_news_data():\n",
    "    path = os.path.join('../../Data', 'processed_data/')\n",
    "    # read news data from 2016 to 2018\n",
    "    NYT_2016 = pd.read_json(path+'NYT/NYT_headline_2016.json')\n",
    "    NYT_2017 = pd.read_json(path+'NYT/NYT_headline_2017.json')\n",
    "    NYT_2018 = pd.read_json(path+'NYT/NYT_headline_2018.json')\n",
    "    Guard_2016 = pd.read_json(path+'Guardian/Guard_headline_2016.json')\n",
    "    Guard_2017 = pd.read_json(path+'Guardian/Guard_headline_2017.json')\n",
    "    Guard_2018 = pd.read_json(path+'Guardian/Guard_headline_2018.json')\n",
    "    ttl = [NYT_2016, NYT_2017, NYT_2018, Guard_2016, Guard_2017, Guard_2018]\n",
    "    ttl_1 = pd.concat(ttl, sort=False)\n",
    "    return ttl_1\n",
    "\n",
    "def stocks(start_date, end_date):\n",
    "# use quandl to acquire nasdaq composite\n",
    "    ndq = quandl.get(\"NASDAQOMX/COMP-NASDAQ\",\n",
    "                        start_date = start_date, \n",
    "                        end_date = end_date)\n",
    "    ndq_df = ndq.reset_index()\n",
    "    return ndq_df\n",
    "\n",
    "def process(news_df, stocks_df):\n",
    "    news_df1 = news_df[['date', 'news_header']]\n",
    "    news_df2 = news_df1.groupby('date').cumcount() + 1\n",
    "    result_3 = result_1.set_index(['date', result_2]).unstack().sort_index(1, level=1)\n",
    "    result_3.columns = ['_'.join(map(str,i)) for i in result_3.columns]\n",
    "    result_3 = result_3.reset_index()\n",
    "    result_3['date'] = pd.to_datetime(result_3['date'])\n",
    "    \n",
    "def process_hd(df):\n",
    "    # Removing punctuations\n",
    "    headline_df= df.iloc[:,1:-3]\n",
    "    headline_df.replace(to_replace=\"[^a-zA-Z]+\", value=\" \", regex=True, inplace=True)\n",
    "    \n",
    "    # Renaming column names for ease of access\n",
    "    list1= [i for i in range(25)]\n",
    "    new_Index=[str(i) for i in list1]\n",
    "    headline_df.columns= new_Index\n",
    "\n",
    "    # Convertng headlines to lower case and remove stop word\n",
    "    for index in new_Index:\n",
    "        headline_df[index]=headline_df[index].apply(lambda x:\" \".join(x for x in str(x).split() if not x in stop_wrd))\n",
    "        headline_df[index]=headline_df[index].str.lower()\n",
    "    # collect headlines into list\n",
    "    headlines = []\n",
    "    for row in range(0,len(headline_df.index)):\n",
    "        headlines.append(' '.join(str(x) for x in headline_df.iloc[row,0:25]))\n",
    "\n",
    "    return headlines\n",
    "\n",
    "def concat_headline(df):\n",
    "    result_1 = df[['date', 'news_header']]\n",
    "    result_2 = result_1.groupby('date').cumcount() + 1\n",
    "    result_3 = result_1.set_index(['date', result_2]).unstack().sort_index(1, level=1)\n",
    "    result_3.columns = ['_'.join(map(str,i)) for i in result_3.columns]\n",
    "    result_3 = result_3.reset_index()\n",
    "    result_3['date'] = pd.to_datetime(result_3['date'])\n",
    "    return result_3\n",
    "\n",
    "df = read_news_data()\n",
    "# filter news section\n",
    "df_1 = filter_sec(df)\n",
    "stocks_df1 = stocks('2016-01-01', '2018-12-31')\n",
    "stocks_df2 = stocks_df1[['Trade Date', 'Index Value']]\n",
    "df_2 = concat_headline(df_1)\n",
    "df_3 = df_2.merge(stocks_df2, left_on='date', right_on='Trade Date')\n",
    "# get the difference of Nasdaq index with previous date\n",
    "df_3['diff'] = df_3['Index Value'].diff()\n",
    "df_3['diff'] = df_3['diff'].fillna(0.0)\n",
    "df_3['diff'] = df_3['diff'].shift(-1)\n",
    "# # column reorder and get 25 news headline\n",
    "cols = df_3.columns.tolist()\n",
    "cols = cols[:26] + cols[-2:]\n",
    "df_5 = df_3[cols]\n",
    "# create a label col to indicate '1' as the index rise and '-1' as index drop\n",
    "df_5['Label'] = df_5['diff'].apply(index_fluc)\n",
    "df_6 = df_5.loc[df_5['Label'] !=0]\n",
    "# divide train and test dataset\n",
    "train = df_6[df_6['date'] < '20180531']\n",
    "test = df_6[df_6['date'] >= '20180601']\n",
    "\n",
    "# train.head()\n",
    "train_headlines = process_hd(train)\n",
    "test_headlines = process_hd(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Support Vector Classification\n",
    "basicvectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "basictrain = basicvectorizer.fit_transform(train_headlines)\n",
    "basicmodel = svm.LinearSVC(C=0.1, class_weight='balanced')\n",
    "basicmodel = basicmodel.fit(basictrain, train[\"Label\"])\n",
    "basictest = basicvectorizer.transform(test_headlines)\n",
    "predictions1_1 = basicmodel.predict(basictest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.56      0.50      0.53        68\n",
      "          1       0.60      0.65      0.62        77\n",
      "\n",
      "avg / total       0.58      0.58      0.58       145\n",
      "\n",
      "0.5793103448275863\n"
     ]
    }
   ],
   "source": [
    "# print(basictrain.shape)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print (classification_report(test[\"Label\"], predictions1_1))\n",
    "print (accuracy_score(test[\"Label\"], predictions1_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Support Vector Classification\n",
    "basicvectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "basictrain1 = basicvectorizer.fit_transform(train_headlines)\n",
    "basicmodel_2 = svm.LinearSVC(C=0.1, class_weight='balanced')\n",
    "basicmodel_2 = basicmodel_2.fit(basictrain1, train[\"Label\"])\n",
    "basictest_2 = basicvectorizer.transform(test_headlines)\n",
    "predictions2 = basicmodel_2.predict(basictest_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.51      0.28      0.36        68\n",
      "          1       0.55      0.77      0.64        77\n",
      "\n",
      "avg / total       0.53      0.54      0.51       145\n",
      "\n",
      "0.5379310344827586\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(test[\"Label\"], predictions2))\n",
    "print (accuracy_score(test[\"Label\"], predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Support Vector Classification\n",
    "basicvectorizer = CountVectorizer(ngram_range=(3,3))\n",
    "basictrain3 = basicvectorizer.fit_transform(train_headlines)\n",
    "basicmodel_3 = svm.LinearSVC(C=0.1, class_weight='balanced')\n",
    "basicmodel_3 = basicmodel_3.fit(basictrain3, train[\"Label\"])\n",
    "basictest_3 = basicvectorizer.transform(test_headlines)\n",
    "predictions3 = basicmodel_3.predict(basictest_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       1.00      0.01      0.03        68\n",
      "          1       0.53      1.00      0.70        77\n",
      "\n",
      "avg / total       0.75      0.54      0.38       145\n",
      "\n",
      "0.5379310344827586\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(test[\"Label\"], predictions3))\n",
    "print (accuracy_score(test[\"Label\"], predictions3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
