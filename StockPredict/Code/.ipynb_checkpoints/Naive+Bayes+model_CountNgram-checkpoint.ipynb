{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem:\n",
    "    2016-12-31 only gets 3 news\n",
    "    Filter news that are less than 10 in order to do stocks prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/denise/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import quandl\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "stop_wrd = set(stopwords.words('english'))\n",
    "\n",
    "def filter_sec(df):\n",
    "    filter_cate = ['N.Y. / Region', 'Today’s Paper', 'Multimedia/Photos', 'U.S.', 'Reader Center', 'Magazine', 'Technology',\n",
    "    'Opinion', 'International Home', 'Job Market', 'Briefing', 'Education', 'World', 'Business', 'Your Money',\n",
    "    'NYT Now', 'Podcasts', 'New York', 'Real Estate', 'Sunday Review', 'Business Day', 'Neediest Cases''T Magazine', 'Times Insider',\n",
    "     'Public Editor', 'Universal'] \n",
    "    df_1 = df[df.section.isin(filter_cate)]\n",
    "    return df_1\n",
    "\n",
    "def index_fluc(value):\n",
    "    if value >0:\n",
    "        return 1\n",
    "    elif value <0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def read_news_data():\n",
    "    path = os.path.join('Data', 'processed_data/')\n",
    "    # read news data from 2016 to 2018\n",
    "    NYT_2016 = pd.read_json(path+'NYT/NYT_headline_2016.json')\n",
    "    NYT_2017 = pd.read_json(path+'NYT/NYT_headline_2017.json')\n",
    "    NYT_2018 = pd.read_json(path+'NYT/NYT_headline_2018.json')\n",
    "    Guard_2016 = pd.read_json(path+'Guardian/Guard_headline_2016.json')\n",
    "    Guard_2017 = pd.read_json(path+'Guardian/Guard_headline_2017.json')\n",
    "    Guard_2018 = pd.read_json(path+'Guardian/Guard_headline_2018.json')\n",
    "    ttl = [NYT_2016, NYT_2017, NYT_2018, Guard_2016, Guard_2017, Guard_2018]\n",
    "    ttl_1 = pd.concat(ttl, sort=False)\n",
    "    return ttl_1\n",
    "\n",
    "def stocks(start_date, end_date):\n",
    "# use quandl to acquire nasdaq composite\n",
    "    ndq = quandl.get(\"NASDAQOMX/COMP-NASDAQ\",\n",
    "                        start_date = start_date, \n",
    "                        end_date = end_date)\n",
    "    ndq_df = ndq.reset_index()\n",
    "    return ndq_df\n",
    "\n",
    "def process(news_df, stocks_df):\n",
    "    news_df1 = news_df[['date', 'news_header']]\n",
    "    news_df2 = news_df1.groupby('date').cumcount() + 1\n",
    "    result_3 = result_1.set_index(['date', result_2]).unstack().sort_index(1, level=1)\n",
    "    result_3.columns = ['_'.join(map(str,i)) for i in result_3.columns]\n",
    "    result_3 = result_3.reset_index()\n",
    "    result_3['date'] = pd.to_datetime(result_3['date'])\n",
    "    \n",
    "def process_hd(df):\n",
    "    # Removing punctuations\n",
    "    headline_df= df.iloc[:,1:-2]\n",
    "    headline_df.replace(to_replace=\"[^a-zA-Z]+\", value=\" \", regex=True, inplace=True)\n",
    "    \n",
    "    # Renaming column names for ease of access\n",
    "    list1= [i for i in range(25)]\n",
    "    new_Index=[str(i) for i in list1]\n",
    "    headline_df.columns= new_Index\n",
    "\n",
    "    # Convertng headlines to lower case and remove stop word\n",
    "    for index in new_Index:\n",
    "        headline_df[index]=headline_df[index].apply(lambda x:\" \".join(x for x in str(x).split() if not x in stop_wrd))\n",
    "        headline_df[index]=headline_df[index].str.lower()\n",
    "    # collect headlines into list\n",
    "    headlines = []\n",
    "    for row in range(0,len(headline_df.index)):\n",
    "        headlines.append(' '.join(str(x) for x in headline_df.iloc[row,0:25]))\n",
    "\n",
    "    return headlines\n",
    "\n",
    "def concat_headline(df):\n",
    "    result_1 = df[['date', 'news_header']]\n",
    "    result_2 = result_1.groupby('date').cumcount() + 1\n",
    "    result_3 = result_1.set_index(['date', result_2]).unstack().sort_index(1, level=1)\n",
    "    result_3.columns = ['_'.join(map(str,i)) for i in result_3.columns]\n",
    "    result_3 = result_3.reset_index()\n",
    "    result_3['date'] = pd.to_datetime(result_3['date'])\n",
    "    return result_3\n",
    "\n",
    "df = read_news_data()\n",
    "# filter news section\n",
    "df_1 = filter_sec(df)\n",
    "stocks_df1 = stocks('2016-01-01', '2018-12-31')\n",
    "stocks_df2 = stocks_df1[['Trade Date', 'Index Value']]\n",
    "df_2 = concat_headline(df_1)\n",
    "df_3 = df_2.merge(stocks_df2, left_on='date', right_on='Trade Date')\n",
    "# get the difference of Nasdaq index with previous date\n",
    "df_3['diff'] = df_3['Index Value'].diff()\n",
    "df_3['diff'] = df_3['diff'].fillna(0.0)\n",
    "# # column reorder and get 25 news headline\n",
    "cols = df_3.columns.tolist()\n",
    "cols = cols[:26] + cols[-1:]\n",
    "df_5 = df_3[cols]\n",
    "# create a label col to indicate '1' as the index rise and '-1' as index drop\n",
    "df_5['Label'] = df_5['diff'].apply(index_fluc)\n",
    "df_6 = df_5.loc[df_5['Label'] !=0]\n",
    "# divide train and test dataset\n",
    "train = df_6[df_6['date'] < '20180531']\n",
    "test = df_6[df_6['date'] >= '20180601']\n",
    "# train.head()\n",
    "train_headlines = process_hd(train)\n",
    "test_headlines = process_hd(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/denise/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "import quandl\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_wrd = set(stopwords.words('english'))\n",
    "\n",
    "def filter_sec(df):\n",
    "    filter_cate = ['N.Y. / Region', 'Today’s Paper', 'Multimedia/Photos', 'U.S.', 'Reader Center', 'Magazine', 'Technology',\n",
    "    'Opinion', 'International Home', 'Job Market', 'Briefing', 'Education', 'World', 'Business', 'Your Money',\n",
    "    'NYT Now', 'Podcasts', 'New York', 'Real Estate', 'Sunday Review', 'Business Day', 'Neediest Cases''T Magazine', 'Times Insider',\n",
    "     'Public Editor', 'Universal'] \n",
    "    df_1 = df[df.section.isin(filter_cate)]\n",
    "    return df_1\n",
    "\n",
    "def index_fluc(value):\n",
    "    if value >0:\n",
    "        return 1\n",
    "    elif value <0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def read_news_data():\n",
    "    path = os.path.join('Data', 'processed_data/')\n",
    "    # read news data from 2016 to 2018\n",
    "    NYT_2016 = pd.read_json(path+'NYT/NYT_headline_2016.json')\n",
    "    NYT_2017 = pd.read_json(path+'NYT/NYT_headline_2017.json')\n",
    "    NYT_2018 = pd.read_json(path+'NYT/NYT_headline_2018.json')\n",
    "    Guard_2016 = pd.read_json(path+'Guardian/Guard_headline_2016.json')\n",
    "    Guard_2017 = pd.read_json(path+'Guardian/Guard_headline_2017.json')\n",
    "    Guard_2018 = pd.read_json(path+'Guardian/Guard_headline_2018.json')\n",
    "    ttl = [NYT_2016, NYT_2017, NYT_2018, Guard_2016, Guard_2017, Guard_2018]\n",
    "    ttl_1 = pd.concat(ttl, sort=False)\n",
    "    return ttl_1\n",
    "\n",
    "def stocks(start_date, end_date):\n",
    "# use quandl to acquire nasdaq composite\n",
    "    ndq = quandl.get(\"NASDAQOMX/COMP-NASDAQ\",\n",
    "                        start_date = start_date, \n",
    "                        end_date = end_date)\n",
    "    ndq_df = ndq.reset_index()\n",
    "    return ndq_df\n",
    "\n",
    "def process(news_df, stocks_df):\n",
    "    news_df1 = news_df[['date', 'news_header']]\n",
    "    news_df2 = news_df1.groupby('date').cumcount() + 1\n",
    "    result_3 = result_1.set_index(['date', result_2]).unstack().sort_index(1, level=1)\n",
    "    result_3.columns = ['_'.join(map(str,i)) for i in result_3.columns]\n",
    "    result_3 = result_3.reset_index()\n",
    "    result_3['date'] = pd.to_datetime(result_3['date'])\n",
    "    \n",
    "def process_hd(df):\n",
    "    # Removing punctuations\n",
    "    headline_df= df.iloc[:,1:-3]\n",
    "    headline_df.replace(to_replace=\"[^a-zA-Z]+\", value=\" \", regex=True, inplace=True)\n",
    "    \n",
    "    # Renaming column names for ease of access\n",
    "    list1= [i for i in range(25)]\n",
    "    new_Index=[str(i) for i in list1]\n",
    "    headline_df.columns= new_Index\n",
    "\n",
    "    # Convertng headlines to lower case and remove stop word\n",
    "    for index in new_Index:\n",
    "        headline_df[index]=headline_df[index].apply(lambda x:\" \".join(x for x in str(x).split() if not x in stop_wrd))\n",
    "        headline_df[index]=headline_df[index].str.lower()\n",
    "    # collect headlines into list\n",
    "    headlines = []\n",
    "    for row in range(0,len(headline_df.index)):\n",
    "        headlines.append(' '.join(str(x) for x in headline_df.iloc[row,0:25]))\n",
    "\n",
    "    return headlines\n",
    "\n",
    "def concat_headline(df):\n",
    "    result_1 = df[['date', 'news_header']]\n",
    "    result_2 = result_1.groupby('date').cumcount() + 1\n",
    "    result_3 = result_1.set_index(['date', result_2]).unstack().sort_index(1, level=1)\n",
    "    result_3.columns = ['_'.join(map(str,i)) for i in result_3.columns]\n",
    "    result_3 = result_3.reset_index()\n",
    "    result_3['date'] = pd.to_datetime(result_3['date'])\n",
    "    return result_3\n",
    "\n",
    "df = read_news_data()\n",
    "# filter news section\n",
    "df_1 = filter_sec(df)\n",
    "stocks_df1 = stocks('2016-01-01', '2018-12-31')\n",
    "stocks_df2 = stocks_df1[['Trade Date', 'Index Value']]\n",
    "df_2 = concat_headline(df_1)\n",
    "df_3 = df_2.merge(stocks_df2, left_on='date', right_on='Trade Date')\n",
    "# get the difference of Nasdaq index with previous date\n",
    "df_3['diff'] = df_3['Index Value'].diff()\n",
    "df_3['diff'] = df_3['diff'].fillna(0.0)\n",
    "# # column reorder and get 25 news headline\n",
    "cols = df_3.columns.tolist()\n",
    "cols = cols[:26] + cols[-2:]\n",
    "df_5 = df_3[cols]\n",
    "# create a label col to indicate '1' as the index rise and '-1' as index drop\n",
    "df_5['Label'] = df_5['diff'].apply(index_fluc)\n",
    "df_6 = df_5.loc[df_5['Label'] !=0]\n",
    "# divide train and test dataset\n",
    "train = df_6[df_6['date'] < '20180531']\n",
    "test = df_6[df_6['date'] >= '20180601']\n",
    "\n",
    "# train.head()\n",
    "train_headlines = process_hd(train)\n",
    "test_headlines = process_hd(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicvectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "basictrain = basicvectorizer.fit_transform(train_headlines)\n",
    "basicmodel = GaussianNB()\n",
    "basicmodel = basicmodel.fit(basictrain.toarray(), train[\"Label\"])\n",
    "\n",
    "basictest = basicvectorizer.transform(test_headlines)\n",
    "predictions = basicmodel.predict(basictest.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 12547)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.59      0.38      0.46        68\n",
      "          1       0.59      0.77      0.67        78\n",
      "\n",
      "avg / total       0.59      0.59      0.57       146\n",
      "\n",
      "0.589041095890411\n"
     ]
    }
   ],
   "source": [
    "print(basictrain.shape)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print (classification_report(test[\"Label\"], predictions))\n",
    "print (accuracy_score(test[\"Label\"], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 200)\n",
      "(605, 201)\n",
      "(146, 200)\n",
      "(146, 201)\n"
     ]
    }
   ],
   "source": [
    "# with stocks price\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "# Normalizing training data.\n",
    "norm_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "dim = basictrain.shape[0]\n",
    "print(basictrain.shape)\n",
    "norm_stock_price = norm_scaler.fit_transform(np.array(train['Index Value']).reshape(dim,1))\n",
    "np_basictrain =basictrain.toarray()\n",
    "word_price_train = np.concatenate((np_basictrain, norm_stock_price), axis=1)\n",
    "print(word_price_train.shape)\n",
    "\n",
    "# Normalizing test data.\n",
    "norm_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "dim = basictest.shape[0]\n",
    "print(basictest.shape)\n",
    "norm_stock_price = norm_scaler.fit_transform(np.array(test['Index Value']).reshape(dim,1))\n",
    "np_basictest =basictest.toarray()\n",
    "word_price_test = np.concatenate((np_basictest, norm_stock_price), axis=1)\n",
    "print(word_price_test.shape)\n",
    "\n",
    "# basictest\n",
    "# fit into model\n",
    "md = GaussianNB()\n",
    "md = md.fit(word_price_train, train[\"Label\"])\n",
    "predictions_test = md.predict(word_price_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.38      0.34      0.36        68\n",
      "          1       0.48      0.53      0.50        78\n",
      "\n",
      "avg / total       0.43      0.44      0.43       146\n",
      "\n",
      "0.4383561643835616\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(test[\"Label\"], predictions_test))\n",
    "print (accuracy_score(test[\"Label\"], predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicvectorizer2 = CountVectorizer(ngram_range=(2,2))\n",
    "basictrain2 = basicvectorizer2.fit_transform(train_headlines)\n",
    "basicmodel2 = GaussianNB()\n",
    "basicmodel2 = basicmodel2.fit(basictrain2.toarray(), train[\"Label\"])\n",
    "basictest2 = basicvectorizer2.transform(test_headlines)\n",
    "predictions2 = basicmodel2.predict(basictest2.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 73371)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.52      0.34      0.41        68\n",
      "          1       0.56      0.73      0.63        78\n",
      "\n",
      "avg / total       0.54      0.55      0.53       146\n",
      "\n",
      "0.547945205479452\n"
     ]
    }
   ],
   "source": [
    "print(basictrain2.shape)\n",
    "\n",
    "# pd.crosstab(test[\"Label\"], predictions2, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "print (classification_report(test[\"Label\"], predictions2))\n",
    "print (accuracy_score(test[\"Label\"], predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicvectorizer3 = CountVectorizer(ngram_range=(3,3))\n",
    "basictrain3 = basicvectorizer3.fit_transform(train_headlines)\n",
    "basicmodel3 = GaussianNB()\n",
    "basicmodel3 = basicmodel3.fit(basictrain3.toarray(), train[\"Label\"])\n",
    "basictest3 = basicvectorizer3.transform(test_headlines)\n",
    "predictions3 = basicmodel3.predict(basictest3.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 90573)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.58      0.68      0.63        68\n",
      "          1       0.67      0.58      0.62        78\n",
      "\n",
      "avg / total       0.63      0.62      0.62       146\n",
      "\n",
      "0.6232876712328768\n"
     ]
    }
   ],
   "source": [
    "print(basictrain3.shape)\n",
    "\n",
    "# pd.crosstab(test[\"Label\"], predictions3, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "print (classification_report(test[\"Label\"], predictions3))\n",
    "print (accuracy_score(test[\"Label\"], predictions3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, norm='l2', ngram_range=(2, 2), max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(train_headlines).toarray()\n",
    "basicmodel5 = GaussianNB()\n",
    "clf = basicmodel5.fit(X_train_tfidf, train[\"Label\"])\n",
    "basictest5 = tfidf.transform(test_headlines)\n",
    "predictions5 = basicmodel5.predict(basictest5.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605, 5000)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.50      0.07      0.13        68\n",
      "          1       0.54      0.94      0.68        78\n",
      "\n",
      "avg / total       0.52      0.53      0.42       146\n",
      "\n",
      "0.5342465753424658\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf.shape)\n",
    "\n",
    "# pd.crosstab(test[\"Label\"], predictions3, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "print (classification_report(test[\"Label\"], predictions5))\n",
    "print (accuracy_score(test[\"Label\"], predictions5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
